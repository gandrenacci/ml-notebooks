{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gianpieroandrenacci/energy-prediction-with-lstm-deep-learning?scriptVersionId=154877806\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction - ðŸ•¥ LSTM Timeseries forecasting with Tensorflow and Keras","metadata":{"id":"-xApCzNY_seZ"}},{"cell_type":"markdown","source":"## <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">LSTM (Long Short-Term Memory) for timeseries</h1> \n\n","metadata":{"id":"w1sWtb7f9Hui"}},{"cell_type":"markdown","source":"In this notebook, we will be exploring LSTM (Long Short-Term Memory) for timeseries forecasting with the help of two popular deep learning libraries: Tensorflow and Keras. \n\n**LSTM is a type of Recurrent Neural Network (RNN) that is well-suited for timeseries** data analysis and prediction tasks. We will learn how to implement LSTM models using Keras and Tensorflow, and use them to forecast future values of a timeseries.","metadata":{"id":"KMGQPcnS-m3v"}},{"cell_type":"markdown","source":"## <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">The Timeseries Dataset - Italian Energy Price</h1> \n","metadata":{"id":"KpBKPoJ5_fem"}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em; \">\n    ðŸ“Œ &nbsp; The PUN (Italian acronym for Prezzo Unico Nazionale, \"National Single Price\") is the wholesale reference price of electricity purchased on the Borsa Elettrica Italiana market (IPEX - Italian Power Exchange). At the Italian Power Exchange, active since 2007 following the entry into force of the\n</div>","metadata":{}},{"cell_type":"markdown","source":"\n\nLegislative Decree governing the liberalization of the electricity market, the transactions between producers and suppliers of electricity are regulated. The PUN therefore represents the national weighted average of the zonal sales prices of electricity for each hour and for each day. The national figure is an amount that is calculated on the average of various factors, and which takes into account the quantities and prices formed in the different areas of Italy and at different times of the day.\n\n\n**How the PUN affects the price of energy**\nThe wholesale price of electricity is established directly on the market based on the trades between the various players involved, i.e. between producers and energy suppliers (who purchase the energy from producers to supply to their end customers). The fluctuations of the PUN are a determining factor in calculating the final costs of energy in the bill. In fact, in the periods in which the PUN increases its value, costs tend to rise, to fall instead when the value of the PUN falls. \n\nEnergy suppliers generally provide for tariffs for the final consumer at a fixed cost or at an indexed cost as regards the price of the energy component. Opting for an indexed price of the energy component means that this will vary over time depending on the performance of the PUN on the Italian Power Exchange. An offer at a fixed price of the energy component, on the other hand, will remain unchanged for a certain period of time depending on the offer chosen, generally for one or two years.\n\n\nThe PUN unit measure is â‚¬/MWh\n\nhttps://www.enel.it/en/supporto/faq/cos-e-il-pun","metadata":{"id":"r71KSbTs_PE4"}},{"cell_type":"markdown","source":"## <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Dataset Details</h1> ","metadata":{"id":"fwXn7A1h_8GQ"}},{"cell_type":"markdown","source":"**PUN** is National Single Price of energy. It is the wholesale reference price of electricity purchased on the Borsa Elettrica Italiana market .\n\n**Foreign Virtual Zone**: point of interconnection with neighboring countries. It includes: France (FRAN), Switzerland (SVIZ), Austria (AUST), Slovenia (SLOV), Slovenia Coupling representing the interconnection dedicated to Market Coupling between Italy and Slovenia (BSP); Corsica (CORS), Corsica AC (COAC), Greece (GREC), France coupling (XFRA), Austria coupling (XAUS), Malta (MALT), Montenegro (MONT) and Italy coupling (COUP).","metadata":{}},{"cell_type":"markdown","source":"# <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Import libraries</h1> ","metadata":{"papermill":{"duration":0.021438,"end_time":"2022-07-25T16:50:25.5379","exception":false,"start_time":"2022-07-25T16:50:25.516462","status":"completed"},"tags":[],"id":"ce60656e"}},{"cell_type":"code","source":"#pip install numpy==1.19","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:32:44.196855Z","iopub.execute_input":"2023-07-24T11:32:44.197383Z","iopub.status.idle":"2023-07-24T11:32:44.202799Z","shell.execute_reply.started":"2023-07-24T11:32:44.197328Z","shell.execute_reply":"2023-07-24T11:32:44.201749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport numpy as np\n\n#import tensorflow as tf\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport calendar\n\n\nimport sklearn\nfrom sklearn import metrics\nimport math\n\nimport datetime\nimport matplotlib.dates as mdates\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom packaging import version\n\nimport os\n#import keras\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","metadata":{"_kg_hide-input":true,"papermill":{"duration":3.198318,"end_time":"2022-07-25T16:50:28.759475","exception":false,"start_time":"2022-07-25T16:50:25.561157","status":"completed"},"tags":[],"id":"42b8108c","execution":{"iopub.status.busy":"2023-07-24T11:32:44.205526Z","iopub.execute_input":"2023-07-24T11:32:44.20623Z","iopub.status.idle":"2023-07-24T11:32:44.217513Z","shell.execute_reply.started":"2023-07-24T11:32:44.20617Z","shell.execute_reply":"2023-07-24T11:32:44.216594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"version.parse(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:32:44.218812Z","iopub.execute_input":"2023-07-24T11:32:44.220057Z","iopub.status.idle":"2023-07-24T11:32:44.234041Z","shell.execute_reply.started":"2023-07-24T11:32:44.220019Z","shell.execute_reply":"2023-07-24T11:32:44.232829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Custom Functions</h1> ","metadata":{"id":"99oo5xUAjmAi"}},{"cell_type":"code","source":"def num_to_time(num):\n    \"\"\" \n    Function convert number to time format\n    num: hour as numneric\n    \"\"\"\n    time = num\n    hours = int(time)\n    # hour\n    if hours == 24:\n        hours = 0\n    # minutes and sec\n    minutes = 0\n    seconds = 0\n    out_time = \"%02d:%02d:%02d\" % (hours, minutes, seconds)\n    \n    return out_time\n\nimport matplotlib.pyplot as plt\n\ndef plot_two_lines(last_period, X1,y1,X2,y2,data1_label,data2_label, ylabel, title,\n                   legend_pos = 'lower right',color1=\"#1f77b4\",\n                   color2=\"#ff7f0e\",linestyle1='solid', linestyle2='dashed'):\n    \"\"\"\n    Plot the train and test data on a single plot.\n\n    Keyword arguments:\n    - last_period (int): The last period in the data.\n    - X1 (array): The X values for the first set of data.\n    - y1 (array): The Y values for the first set of data.\n    - X2 (array): The X values for the second set of data.\n    - y2 (array): The Y values for the second set of data.\n    - data1_label (str): The label for the first set of data.\n    - data2_label (str): The label for the second set of data.\n    - ylabel (str): The label for the Y axis.\n    - title (str): The title of the pl\n    - legend_pos : leged position\n    \"\"\"\n    #if we want only a slice of the data\n    if last_period != 0:\n        X1 = X1[-last_period:] \n        y1 = y1.iloc[-last_period:]\n        X2 = X2[-last_period:] \n        y2 = y2.iloc[-last_period:]\n\n    # Create the plot\n    plt.figure(figsize=(10, 7))\n    plt.plot(X1, y1, color=color1, label=data1_label,linestyle =linestyle1 )\n    plt.plot(X2, y2, color=color2, label=data2_label, linestyle = linestyle2)\n    plt.ylabel(ylabel, fontsize=14)\n    plt.grid(axis='x')\n    plt.legend(fontsize=14, loc=legend_pos)\n\n    # Remove the top and right borders\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n\n    # Remove the bottom and left borders\n    plt.gca().spines['bottom'].set_visible(False)\n    plt.gca().spines['left'].set_visible(False)\n\n    plt.title(title, fontsize=16)\n    plt.show()  \n\n\ndef error_metrics(y_test, y_pred):\n    \"\"\"\n    Calculate the most common forecast error metrics\n    y_test: variable name y\n    y_pred: variable name yhat\n    \"\"\"\n    #R2 - coefficient of determination\n    R2 = sklearn.metrics.r2_score(y_test, y_pred)\n\n    #Mean squared error\n    MSE = sklearn.metrics.mean_squared_error(y_test, y_pred)\n\n    #Root Mean squared error\n    RMSE = math.sqrt(MSE)\n\n    #Mean absolute error\n    MAE =  sklearn.metrics.mean_absolute_error(y_test, y_pred)\n\n    #Median absolute error \n    MdAE = sklearn.metrics.median_absolute_error(y_test, y_pred)\n\n    #Mean percentage error\n    MAPE = sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred)\n\n    if R2.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n        R2 = tf.reduce_mean(R2)\n        MSE  = tf.reduce_mean(MSE)\n        RMSE = tf.reduce_mean(RMSE)\n        MAE = tf.reduce_mean(MAE)\n        MdAE = tf.reduce_mean(MdAE)\n        MAPE = tf.reduce_mean(MAPE)\n\n    error_dic =  {\n          \"R2\": R2,\n          \"mse\": MSE,\n          \"rmse\": RMSE,\n          \"mae\": MAE,\n          \"MdAE\": MdAE,\n          \"MAPE\": MAPE\n          }\n\n    \n    print('R2: %.3f' % R2)\n    print('MSE (Mean squared error): ' f\"{MSE:,.0f}\")\n    print('RMSE (Root mean squared error): ' f\"{RMSE:,.0f}\")\n    print('MAE (Mean absolute error): ' f\"{MAE:,.0f}\")\n    print('MdAE (Median absolute error ): ' f\"{MdAE:,.0f}\")\n    print('MAPE (Mean percentage error): ' f\"{MAPE:,.2%}\")\n   \n    \n    return error_dic    ","metadata":{"id":"GADdKtyYj3DI","execution":{"iopub.status.busy":"2023-07-24T11:32:44.235797Z","iopub.execute_input":"2023-07-24T11:32:44.236229Z","iopub.status.idle":"2023-07-24T11:32:44.257488Z","shell.execute_reply.started":"2023-07-24T11:32:44.236192Z","shell.execute_reply":"2023-07-24T11:32:44.256404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Load the dataset</h1> ","metadata":{"id":"f_k41SLijpfH"}},{"cell_type":"markdown","source":"In this section, we will start our analysis by loading the dataset into our Python environment. For this purpose, we are using the widely popular data analysis library in Python, **pandas**. We load the \"energy_pun_main_zones.csv\" file which resides in our Kaggle input directory.\n\nFirst, we'll use the pandas **read_csv()** function to import the .csv file. Notice that we are parsing the 'DATE' column as date type right at the time of loading. This is a handy trick to ensure our date data is in the right format from the outset, making subsequent time series analysis more convenient.\n\nOnce the data is loaded into a pandas dataframe (df), we proceed to drop the 'IDX' column. This step is based on the understanding that 'IDX' is an identifier column, not needed for our analysis.\n\nFinally, **we check the completeness of the dataset by using the info() function**, which provides a concise summary of the dataframe including the number of non-null entries in each column. This initial check for missing values is an important step in data cleaning, as missing values can potentially affect our analysis results.\n\nLet's proceed with loading our dataset.","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"5Ndq18k2ElzK","execution":{"iopub.status.busy":"2023-07-24T11:32:44.261143Z","iopub.execute_input":"2023-07-24T11:32:44.262237Z","iopub.status.idle":"2023-07-24T11:32:44.274795Z","shell.execute_reply.started":"2023-07-24T11:32:44.2622Z","shell.execute_reply":"2023-07-24T11:32:44.273364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read file and parse date\ndf = pd.read_csv(\"/kaggle/input/energy-pun-main-zones/energy_pun_main_zones.csv\",parse_dates = ['DATE'])","metadata":{"id":"7f_PMl6H-fSn","execution":{"iopub.status.busy":"2023-07-24T11:32:44.278275Z","iopub.execute_input":"2023-07-24T11:32:44.279098Z","iopub.status.idle":"2023-07-24T11:32:44.550282Z","shell.execute_reply.started":"2023-07-24T11:32:44.27906Z","shell.execute_reply":"2023-07-24T11:32:44.548915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"id":"NGKuT0p6juTT","outputId":"a209219f-473e-4115-b217-730a6e877a70","execution":{"iopub.status.busy":"2023-07-24T11:32:44.553185Z","iopub.execute_input":"2023-07-24T11:32:44.553909Z","iopub.status.idle":"2023-07-24T11:32:44.575523Z","shell.execute_reply.started":"2023-07-24T11:32:44.55386Z","shell.execute_reply":"2023-07-24T11:32:44.574394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop id , we don't need it\ndf = df.drop(columns = 'IDX')","metadata":{"id":"t3xOchqqjus4","execution":{"iopub.status.busy":"2023-07-24T11:32:44.578902Z","iopub.execute_input":"2023-07-24T11:32:44.579378Z","iopub.status.idle":"2023-07-24T11:32:44.590962Z","shell.execute_reply.started":"2023-07-24T11:32:44.579336Z","shell.execute_reply":"2023-07-24T11:32:44.589555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make all columns lowercase\ndf.columns = df.columns.str.lower()","metadata":{"id":"DamnQlCsj8Ln","execution":{"iopub.status.busy":"2023-07-24T11:32:44.592711Z","iopub.execute_input":"2023-07-24T11:32:44.593093Z","iopub.status.idle":"2023-07-24T11:32:44.603335Z","shell.execute_reply.started":"2023-07-24T11:32:44.593056Z","shell.execute_reply":"2023-07-24T11:32:44.602253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if there are nulls\ndf.info()","metadata":{"id":"ykqGUFmvj9gP","outputId":"cc09fd78-8b01-4977-908b-088cfe707ce0","execution":{"iopub.status.busy":"2023-07-24T11:32:44.605955Z","iopub.execute_input":"2023-07-24T11:32:44.606277Z","iopub.status.idle":"2023-07-24T11:32:44.628342Z","shell.execute_reply.started":"2023-07-24T11:32:44.606238Z","shell.execute_reply":"2023-07-24T11:32:44.626902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:130%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Select only PUN</p>","metadata":{"id":"Mde2Q5gJAqal"}},{"cell_type":"markdown","source":"In this section, we are narrowing our focus to only three columns from the entire dataset: **'date', 'hour', and 'pun'**. This is done to simplify our dataset and keep our analysis focused on the necessary information.\n\nUpon further inspection of the 'hour' column, we noticed some inconsistencies: some rows had values greater than 24, which is not applicable to our **24-hour format**. These values are due to the transition to **daylight saving time**. To keep our dataset consistent, we filter out these rows, ensuring the 'hour' column only contains values less than or equal to 24.\n\nNext, we apply the custom function num_to_time on the 'hour' column, converting numerical representations into time format. We also take care of instances where the hour is '24', treating them as '0' (midnight) for consistency in our time series analysis.\n\nThe 'hour' column is then converted to an integer data type for ease of further processing and analysis.\n\nNext, we create a new 'date_time' column by **concatenating 'date' and 'time'**. We ensure this column is of type datetime by using the pandas to_datetime() function.\n\nThe 'date' column is also converted into a datetime format if it wasn't already. This format is preferred as it enables more straightforward date-related operations.\n\nLastly, we set **'date_time' as the index** of our dataframe and sort our dataframe in ascending order. This allows for a more intuitive and efficient time series analysis going forward, as data indexed and sorted by time makes analyses and visualizations more manageable.\n\nLet's proceed with these transformations to refine our dataset for further analysis.","metadata":{}},{"cell_type":"code","source":"df = df[['date','hour','pun']]","metadata":{"id":"AebzS4GLkCEJ","execution":{"iopub.status.busy":"2023-07-24T11:32:44.630127Z","iopub.execute_input":"2023-07-24T11:32:44.630576Z","iopub.status.idle":"2023-07-24T11:32:44.637472Z","shell.execute_reply.started":"2023-07-24T11:32:44.630539Z","shell.execute_reply":"2023-07-24T11:32:44.636155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"id":"upOzrRF2kW7J","outputId":"55f9a423-5e63-40f9-b853-d4f28ad83d90","execution":{"iopub.status.busy":"2023-07-24T11:32:44.639498Z","iopub.execute_input":"2023-07-24T11:32:44.639939Z","iopub.status.idle":"2023-07-24T11:32:44.657009Z","shell.execute_reply.started":"2023-07-24T11:32:44.639848Z","shell.execute_reply":"2023-07-24T11:32:44.655831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are some rows with hour greater than 24\ndf[df['hour']  > 24]","metadata":{"id":"P0WfA3NYkZCZ","outputId":"f23d6b3b-27fd-4754-c2dc-787baeb93c13","execution":{"iopub.status.busy":"2023-07-24T11:32:44.658423Z","iopub.execute_input":"2023-07-24T11:32:44.659323Z","iopub.status.idle":"2023-07-24T11:32:44.678844Z","shell.execute_reply.started":"2023-07-24T11:32:44.659262Z","shell.execute_reply":"2023-07-24T11:32:44.677619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It is for daylight saving time. We can delete these rows\n# out of range hour filter\n\ndf =df[df['hour']  < 25]","metadata":{"id":"W4XPCW7Qkbqi","execution":{"iopub.status.busy":"2023-07-24T11:32:44.680803Z","iopub.execute_input":"2023-07-24T11:32:44.681812Z","iopub.status.idle":"2023-07-24T11:32:44.693565Z","shell.execute_reply.started":"2023-07-24T11:32:44.681764Z","shell.execute_reply":"2023-07-24T11:32:44.692637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert number to time format\ndf['time'] = df['hour'].apply(num_to_time)\n\n# convert 24 to 0 (midnight)\ndf['hour'] = df['hour'].apply(lambda x: 0 if x == 24  else x)\n\n# convert to int\ndf['hour'] = df['hour'].astype(int) \n\n# convert date and time to datetime\ndf['date_time'] = pd.to_datetime(df.date.astype(str) + ' ' + df.time.astype(str))\ndf['date_time'] = pd.to_datetime(df['date_time'])\n\n#convert date to date\ndf['date'] = pd.to_datetime(df['date'])\n\n# set index and sort ascending\ndf = df.set_index('date_time')\ndf = df.sort_index(ascending = True)","metadata":{"id":"zo3TfOcHkdhy","outputId":"58b14178-0cc0-494a-a54b-b9bb13506019","execution":{"iopub.status.busy":"2023-07-24T11:32:44.700851Z","iopub.execute_input":"2023-07-24T11:32:44.701962Z","iopub.status.idle":"2023-07-24T11:32:46.417328Z","shell.execute_reply.started":"2023-07-24T11:32:44.701923Z","shell.execute_reply":"2023-07-24T11:32:46.416167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"id":"ZAcAF0KFkm-B","outputId":"22fb457b-2dcc-4955-a88f-9892e91d60eb","execution":{"iopub.status.busy":"2023-07-24T11:32:46.418905Z","iopub.execute_input":"2023-07-24T11:32:46.419283Z","iopub.status.idle":"2023-07-24T11:32:46.43245Z","shell.execute_reply.started":"2023-07-24T11:32:46.419243Z","shell.execute_reply":"2023-07-24T11:32:46.431236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Preprocess the data</h1>  ","metadata":{"id":"iX9KTinLmjVQ"}},{"cell_type":"code","source":"## Final Dataframe\npun = df[['pun']].copy()\npun","metadata":{"id":"VU3MdJp8koo6","outputId":"9e4f1e63-a6a7-4907-d7cc-4099b36fa15b","execution":{"iopub.status.busy":"2023-07-24T11:32:46.434418Z","iopub.execute_input":"2023-07-24T11:32:46.435322Z","iopub.status.idle":"2023-07-24T11:32:46.451707Z","shell.execute_reply.started":"2023-07-24T11:32:46.435224Z","shell.execute_reply":"2023-07-24T11:32:46.450544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-size:130%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Baseline</p>","metadata":{"id":"j8g0E7_8okZT"}},{"cell_type":"markdown","source":"**We are tracking data from past 672 timestamps (4 weeks in hours). This data will be used to predict the energy price in the next hour**\n\n\nIn this section, we will conduct a baseline evaluation of our data. Our primary objective here is to predict the energy price for the next hour using the past 672 timestamps, equivalent to 4 weeks of hourly data.\n\nTo begin, we split our data into a **training-validation set** and a test set, using a 95-5 split. We assign the majority of the data to the training-validation set to adequately train our models. The remaining 5% will serve as our test set for evaluating the model's performance on unseen data.\n\nOur baseline model will use **a naive method of timeseries forecasting**, predicting the next hour's energy price as the energy price 24 hours prior. This method creates a lagged dataset where the current 'pun' value is predicted by the 'pun' value from the previous day at the same time.\n","metadata":{"id":"pkQvJWzk7svF"}},{"cell_type":"code","source":"past = 672","metadata":{"id":"Ux23pQycpJn4","execution":{"iopub.status.busy":"2023-07-24T11:32:46.453737Z","iopub.execute_input":"2023-07-24T11:32:46.454187Z","iopub.status.idle":"2023-07-24T11:32:46.460155Z","shell.execute_reply.started":"2023-07-24T11:32:46.454095Z","shell.execute_reply":"2023-07-24T11:32:46.458855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create test split\n\nsplit_fraction_test = 0.95\ntrain_val_split = int(split_fraction_test * int(pun.shape[0]))\n\ntrain_data_val = pun.iloc[0 : train_val_split]\ntest_data = pun.iloc[train_val_split:]\n\nlen(train_data_val), len(test_data)","metadata":{"id":"fezbaXHnpPag","outputId":"426e66a3-1b0f-407b-e8ff-40234dcdd5ec","execution":{"iopub.status.busy":"2023-07-24T11:32:46.4618Z","iopub.execute_input":"2023-07-24T11:32:46.462936Z","iopub.status.idle":"2023-07-24T11:32:46.474143Z","shell.execute_reply.started":"2023-07-24T11:32:46.462897Z","shell.execute_reply":"2023-07-24T11:32:46.473025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We create a baseline for comparing our model. We assume that the PUN of next hour will be the same value of the day before at the same time of the day.**","metadata":{}},{"cell_type":"code","source":"# This code creates a baseline model for timeseries forecasting.\ny_base_pred = pd.DataFrame()\ny_base_pred.index = test_data.index\nshift = 24\n# sets the value of the predicted pun timeseries to be the same as the pun timeseries \n# shifted by 24 time steps (i.e., one day if the data is daily).\n\ny_base_pred['pun'] = test_data['pun'].shift(shift)\n\n# removes any rows from y_base_pred that contain missing values, +\n# which are created because the first 24 time steps\n\ny_base_pred.dropna(inplace = True)","metadata":{"id":"BNharloldSaQ","execution":{"iopub.status.busy":"2023-07-24T11:32:46.475788Z","iopub.execute_input":"2023-07-24T11:32:46.476263Z","iopub.status.idle":"2023-07-24T11:32:46.487917Z","shell.execute_reply.started":"2023-07-24T11:32:46.476212Z","shell.execute_reply":"2023-07-24T11:32:46.486879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_base_pred","metadata":{"id":"6sIltbObfYQA","outputId":"24c0a8b6-6fac-4b6c-fad5-c347de6cedf0","execution":{"iopub.status.busy":"2023-07-24T11:32:46.489855Z","iopub.execute_input":"2023-07-24T11:32:46.490467Z","iopub.status.idle":"2023-07-24T11:32:46.504234Z","shell.execute_reply.started":"2023-07-24T11:32:46.490418Z","shell.execute_reply":"2023-07-24T11:32:46.502887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline vs true values\n\nplot_two_lines(past, y_base_pred.index, test_data['pun'].iloc[shift:],y_base_pred.index, y_base_pred ,\\\n               ylabel=\"Pun\",data1_label= 'Test Data', data2_label = 'Pred Data' ,\n               title=\"Baseline Prediction VS Actual\", legend_pos = 'upper left',\n               color1 = '#001a4d', color2 = \"#ffa83d\",\n               )","metadata":{"id":"3Mpi1Fwfn7VW","outputId":"62bbbe5e-5ccc-4e12-a7e3-d6de1f8b9703","execution":{"iopub.status.busy":"2023-07-24T11:32:46.505859Z","iopub.execute_input":"2023-07-24T11:32:46.506278Z","iopub.status.idle":"2023-07-24T11:32:46.852639Z","shell.execute_reply.started":"2023-07-24T11:32:46.506241Z","shell.execute_reply":"2023-07-24T11:32:46.851262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error metrics\nerror_dic = error_metrics(test_data['pun'].iloc[shift:], y_base_pred)","metadata":{"id":"HJt8nxLPoxLK","outputId":"80a7c717-aeb3-47bc-d559-d68eff4166d8","execution":{"iopub.status.busy":"2023-07-24T11:32:46.854415Z","iopub.execute_input":"2023-07-24T11:32:46.855105Z","iopub.status.idle":"2023-07-24T11:32:46.873462Z","shell.execute_reply.started":"2023-07-24T11:32:46.855063Z","shell.execute_reply":"2023-07-24T11:32:46.87237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After generating the predictions, we compute several **error metrics** including R-squared (R2), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Median Absolute Error (MdAE), and Mean Percentage Error (MAPE). These metrics provide different perspectives on the model's prediction accuracy.\n\n\nThese values provide a reference point, or a \"**baseline**\", against which we can measure the performance of more sophisticated forecasting models. Remember, the goal of introducing more complexity to our model is to reduce these error metrics further, indicating improved forecast accuracy.\n\nNow that we've established our baseline, let's proceed to develop more advanced predictive models.","metadata":{}},{"cell_type":"markdown","source":"# <h1 style=\"background-color:#336600;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Timeseries forecasting</h1> ","metadata":{"id":"JQB1k_mCvjfO"}},{"cell_type":"markdown","source":"## <p style=\"font-size:100%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Prepare the dataset for training</p>","metadata":{}},{"cell_type":"markdown","source":"In this section, we're preparing our data for **LSTM (Long Short-Term Memory)** model, a type of recurrent neural network that is commonly used for time series prediction tasks.\n\nFirst, we further **split our training-validation** data into a separate training and validation set, using an 80-20 split. The training set will be used to train the model, while the validation set will be used to tune the model parameters and prevent overfitting.\n\nNext, we normalize our data. Normalization is a common preprocessing step for deep learning models, especially for neural networks, as it scales all input features to the same range. This makes the model less sensitive to the scale of features and helps to speed up the training process.\n\nWe **normalize** the training, validation, and test datasets using the maximum and minimum values from the training dataset. It's important to note that we only use the maximum and minimum of the training set to avoid data leakage, which refers to the usage of information from the test set in the model training process, potentially leading to overoptimistic results.\n\nBy the end of this section, our data is adequately prepared and normalized for the LSTM model. This marks an important step in our time series analysis, as we are transitioning from simpler prediction techniques to more complex and powerful deep learning models. Let's proceed to train our LSTM model with this prepared data.","metadata":{}},{"cell_type":"code","source":"# split train and validation\nsplit_fraction_train = 0.8\ntrain_split = int(split_fraction_train * len(train_data_val))\n\ntrain_data = train_data_val.iloc[0 : train_split]\nval_data = train_data_val.iloc[train_split:]\n\nlen(train_data), len(val_data)","metadata":{"id":"V63egtmGccAK","outputId":"79a543ff-e58f-49e1-d663-8fcc4b4eba6d","execution":{"iopub.status.busy":"2023-07-24T11:32:46.8759Z","iopub.execute_input":"2023-07-24T11:32:46.876819Z","iopub.status.idle":"2023-07-24T11:32:46.885902Z","shell.execute_reply.started":"2023-07-24T11:32:46.876777Z","shell.execute_reply":"2023-07-24T11:32:46.884589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize train, validation and test date indexes \n\ntrain_data.index, val_data.index, test_data.index","metadata":{"id":"heap0QNFc1TJ","outputId":"4bed50f6-8a89-4ac2-fc24-4d72793c70e7","execution":{"iopub.status.busy":"2023-07-24T11:32:46.888315Z","iopub.execute_input":"2023-07-24T11:32:46.889535Z","iopub.status.idle":"2023-07-24T11:32:46.904036Z","shell.execute_reply.started":"2023-07-24T11:32:46.889486Z","shell.execute_reply":"2023-07-24T11:32:46.902159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:100%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Normalize</p>","metadata":{}},{"cell_type":"markdown","source":"This code performs min-max normalization on the training, validation, and test data before training the LSTM model. Min-max normalization scales the data so that all values are between 0 and 1, with the minimum value of the data being mapped to 0 and the maximum value being mapped to 1. ","metadata":{}},{"cell_type":"code","source":"# Normalize the data  the data \ntrain_max = train_data.max()\ntrain_min = train_data.min()\n\ntrain_data_norm = (train_data - train_min)  / (train_max - train_min)\nval_data_norm = (val_data - train_min) / (train_max - train_min)\ntest_data_norm = (test_data - train_min) / (train_max - train_min)","metadata":{"id":"SuCFMrtFfjdE","execution":{"iopub.status.busy":"2023-07-24T11:32:46.907191Z","iopub.execute_input":"2023-07-24T11:32:46.907781Z","iopub.status.idle":"2023-07-24T11:32:46.920609Z","shell.execute_reply.started":"2023-07-24T11:32:46.907742Z","shell.execute_reply":"2023-07-24T11:32:46.919035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_norm.head(3)","metadata":{"id":"UoJm9WzFhB_f","outputId":"f9e32b62-07ab-4696-ac33-20e352cb4dcf","execution":{"iopub.status.busy":"2023-07-24T11:32:46.922391Z","iopub.execute_input":"2023-07-24T11:32:46.922761Z","iopub.status.idle":"2023-07-24T11:32:46.93382Z","shell.execute_reply.started":"2023-07-24T11:32:46.922723Z","shell.execute_reply":"2023-07-24T11:32:46.932589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:100%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Build the train, validation and test set with Keras</p>","metadata":{}},{"cell_type":"markdown","source":"In this section, we are preparing our train and validation datasets to be compatible with the Keras API for model training. We're employing a method of data preparation called batch training, which is commonly used when training deep learning models.\n\nWe set our **learning rate** to 0.001 and batch size to 256. The learning rate is a hyperparameter that determines how much the weights of our network will change in each step of learning. The batch size is the number of training examples used in one iteration.\n\nThe '**sequence_length**' parameter denotes the number of past time steps (or 'lags') that will be used as input features to predict the next time step. In this case, we set it equal to 'past', which represents the number of previous timestamps we're using for our predictions.\n\nWe prepare our 'X_train' and 'y_train' data arrays. 'X_train' represents the input features for our model, while 'y_train' represents the target variable we're trying to predict (the 'pun' variable in our case).\n\nWe use the **'timeseries_dataset_from_array'** function from Keras preprocessing module to generate a time-series dataset suitable for training our LSTM model. This function will automatically transform our input data and labels into a format that is compatible with time series prediction tasks.\n\nWe repeat the same process for our validation data to create 'dataset_val'.\n\nFinally, to verify that our data is correctly formatted, we print the shape of one batch of inputs and targets from our training set. The shapes should match our expectations, with each input batch having a shape of (batch size, sequence_length, number of features) and each target batch having a shape of (batch size, 1).\n\nNow that we've transformed our data into a suitable format for **LSTM model**, we're ready to proceed with the model building and training process.","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.001\nbatch_size = 256\nsequence_length = int(past)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:32:46.935552Z","iopub.execute_input":"2023-07-24T11:32:46.936189Z","iopub.status.idle":"2023-07-24T11:32:46.941088Z","shell.execute_reply.started":"2023-07-24T11:32:46.936152Z","shell.execute_reply":"2023-07-24T11:32:46.939884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the labels start from the startnd observation  \n# beacasue prediction is done with n past timesteps\nstart = past \n\n#end = start + train_split\nX_train = train_data_norm.values\ny_train = train_data_norm[['pun']].iloc[:].values\n\n\nprint(len(X_train)), print(len(y_train))","metadata":{"id":"WpFv8rXNc69Z","outputId":"08307197-ced0-4288-9cc5-d5cd78f7164c","execution":{"iopub.status.busy":"2023-07-24T11:32:46.942729Z","iopub.execute_input":"2023-07-24T11:32:46.943432Z","iopub.status.idle":"2023-07-24T11:32:46.957214Z","shell.execute_reply.started":"2023-07-24T11:32:46.943393Z","shell.execute_reply":"2023-07-24T11:32:46.955944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The timeseries_dataset_from_array function takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as length of the sequences to produce batches of sub-timeseries inputs and targets sampled from the main timeseries.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create train keras dataset\n\ndataset_train =  keras.preprocessing.timeseries_dataset_from_array(\n    X_train,\n    y_train,\n    sequence_length=sequence_length,\n    batch_size=batch_size\n   \n)","metadata":{"id":"tHUOpzRdeMYx","execution":{"iopub.status.busy":"2023-07-24T11:32:46.958934Z","iopub.execute_input":"2023-07-24T11:32:46.959664Z","iopub.status.idle":"2023-07-24T11:32:49.542729Z","shell.execute_reply.started":"2023-07-24T11:32:46.959627Z","shell.execute_reply":"2023-07-24T11:32:49.541597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)","metadata":{"id":"6_88qfzgicKO","outputId":"72519415-130d-4b45-b89a-58a27b3490b5","execution":{"iopub.status.busy":"2023-07-24T11:32:49.544244Z","iopub.execute_input":"2023-07-24T11:32:49.544656Z","iopub.status.idle":"2023-07-24T11:32:49.736063Z","shell.execute_reply.started":"2023-07-24T11:32:49.544615Z","shell.execute_reply":"2023-07-24T11:32:49.734712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation tf.keras dataset\n\nX_val = val_data_norm.values\ny_val = val_data_norm[['pun']].iloc[:].values\n\ndataset_val = tf.keras.preprocessing.timeseries_dataset_from_array(\n    X_val,\n    y_val,\n    sequence_length=sequence_length,\n    batch_size=batch_size,\n)\n\n\nfor batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)","metadata":{"id":"EIRJs8fAitMH","outputId":"c57f7a26-af1c-466d-80f0-67aa51f0d8f4","execution":{"iopub.status.busy":"2023-07-24T11:32:49.739229Z","iopub.execute_input":"2023-07-24T11:32:49.740564Z","iopub.status.idle":"2023-07-24T11:32:49.890725Z","shell.execute_reply.started":"2023-07-24T11:32:49.740519Z","shell.execute_reply":"2023-07-24T11:32:49.889586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_val)), print(len(y_val))\n","metadata":{"id":"a_sVmjSwM14R","outputId":"d95c40e8-dea9-4c2a-c5a8-839445e66a94","execution":{"iopub.status.busy":"2023-07-24T11:32:49.891964Z","iopub.execute_input":"2023-07-24T11:32:49.892549Z","iopub.status.idle":"2023-07-24T11:32:49.901557Z","shell.execute_reply.started":"2023-07-24T11:32:49.892505Z","shell.execute_reply":"2023-07-24T11:32:49.900228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail(1)","metadata":{"id":"DMnT936ANZHX","outputId":"450dd9f1-c0e7-43cf-b0f2-4317790507d9","execution":{"iopub.status.busy":"2023-07-24T11:32:49.903448Z","iopub.execute_input":"2023-07-24T11:32:49.904261Z","iopub.status.idle":"2023-07-24T11:32:49.915833Z","shell.execute_reply.started":"2023-07-24T11:32:49.904221Z","shell.execute_reply":"2023-07-24T11:32:49.914571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.head(1)","metadata":{"id":"HSnGg1HCsz8F","outputId":"35e483b6-c9dd-40b3-c994-d4de33806f50","execution":{"iopub.status.busy":"2023-07-24T11:32:49.917807Z","iopub.execute_input":"2023-07-24T11:32:49.918775Z","iopub.status.idle":"2023-07-24T11:32:49.929984Z","shell.execute_reply.started":"2023-07-24T11:32:49.918736Z","shell.execute_reply":"2023-07-24T11:32:49.928405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"font-size:130%; background-color:#336600;color:#fff;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Build and train the LSTM model</p>","metadata":{"id":"Bv9Jcp7XUbjn"}},{"cell_type":"markdown","source":"In this section, we set up and train our **LSTM (Long Short-Term Memory) model** using the Keras API. Our LSTM model is a type of recurrent neural network that's well-suited to time series data as it can learn long-term dependencies, which is useful for our task of predicting the 'pun' variable based on past observations.\n\nFirst, we set the number of training epochs to 10. An epoch is a full pass through the entire training dataset.\n\nWe then define the **architecture of our LSTM model**:\n\n1. The Input layer accepts data of the shape that corresponds to the number of past steps and features in our dataset.\n2. The first LSTM layer with 64 units returns sequences, meaning it outputs an hidden state for each input time step.\n3. A Dropout layer follows, which helps prevent overfitting by randomly setting a fraction (20% here) of input units to 0 at each update during training time.\n4. Another LSTM layer follows, this time with 32 units and it does not return sequences.\n5. This is followed by another Dropout layer for regularisation.\n6. Finally, a Dense layer (or fully connected layer) with a single unit and a ReLU (Rectified Linear Unit) activation function. This is our output layer, which will provide the predicted 'pun' value.\n\nAfter defining the architecture, we **compile the model**. During model compilation, we specify the optimizer and the loss function. We use the RMSprop optimizer with the previously defined learning rate, and Mean Absolute Error (MAE) as our loss function, which is a common choice for regression problems.\n\nWe print the summary of our model architecture, which gives a neat tabular overview of the model layers, their type, output shape and the number of parameters.\n\nOnce our LSTM model is set up, the next step is to **train the model** using our training and validation datasets. Remember that the aim of training this LSTM model is to achieve lower error metrics than our baseline model.\n\n","metadata":{}},{"cell_type":"code","source":"epochs = 10\n\n# Define the model architecture\ninputs = tf.keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\nx = tf.keras.layers.LSTM(64, return_sequences=True)(inputs)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.LSTM(32)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(1, activation='relu')(x)\n\n# Build and compile the model\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate), loss=\"mae\")\n\n# Print the model summary\nmodel.summary()\n","metadata":{"id":"zR6U_B5aUetd","outputId":"0e150b07-acec-48b0-d6ae-9d503eacf870","execution":{"iopub.status.busy":"2023-07-24T11:32:49.937188Z","iopub.execute_input":"2023-07-24T11:32:49.937543Z","iopub.status.idle":"2023-07-24T11:32:50.591288Z","shell.execute_reply.started":"2023-07-24T11:32:49.937516Z","shell.execute_reply":"2023-07-24T11:32:50.590467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This part of the code sets up callbacks for our model and commences the training process.\n\nThe line **path_checkpoint** = \"model_checkpoint.h5\" sets the file path where the model checkpoints will be saved during training. The '.h5' extension indicates that the model is saved in the HDF5 file format, which is a common format for storing large quantities of numerical data like the weights of a trained neural network.\n\nThe **tf.keras.callbacks.EarlyStopping** is a callback function that stops training when a monitored quantity has stopped improving. In this case, we monitor the validation loss (\"val_loss\") and stop the training if it hasn't decreased (min_delta=0) for 5 consecutive epochs (**patience**=5). This is done to prevent overfitting and reduce computational costs.\n\nThe tf.keras.callbacks.ModelCheckpoint callback saves the model or weights (in this case, only weights) at some interval, so the model or weights can be loaded later to continue the training from the state saved. Here, it saves the weights of the model that has the **lowest validation loss** (monitor=\"val_loss\" and save_best_only=True).\n\nFinally, **model.fit** starts the training of the model for a specified number of epochs (iterations on a dataset). It takes the training data, validation data, and the callbacks as arguments. The training data is fed to the model, and the model learns to make accurate predictions. The validation data is used to evaluate the model's performance at the end of each epoch. The callbacks we defined earlier are used during training to implement early stopping and save the model weights. The training process outputs a History object, which is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).","metadata":{}},{"cell_type":"code","source":"path_checkpoint = \"model_checkpoint.h5\"\nes_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n\nmodelckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    monitor=\"val_loss\",\n    filepath=path_checkpoint,\n    verbose=1,\n    save_weights_only=True,\n    save_best_only=True,\n)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=epochs,\n    validation_data=dataset_val,\n    callbacks=[es_callback, modelckpt_callback],\n)","metadata":{"id":"axv-IsePUjv0","outputId":"003ad28d-202e-40fb-999f-cd18b250c042","execution":{"iopub.status.busy":"2023-07-24T11:32:50.592446Z","iopub.execute_input":"2023-07-24T11:32:50.593004Z","iopub.status.idle":"2023-07-24T11:34:02.399667Z","shell.execute_reply.started":"2023-07-24T11:32:50.592972Z","shell.execute_reply":"2023-07-24T11:34:02.398607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model","metadata":{"id":"ppbAOEK_I00k"}},{"cell_type":"markdown","source":"This part of the code evaluates the trained LSTM model on both the training-validation and the test datasets.\n\nThe tf.keras.preprocessing.timeseries_dataset_from_array function is used again to create datasets for both the training-validation and test data. The 'past' variable defines how many previous time steps we use as input features for our model.\n\n**The LSTM model is then used to make predictions** (model.predict(X_test)) on the test dataset.\n\nThe predicted values (y_pred) are initially in the normalized range (between 0 and 1), as the model was trained on normalized data. To interpret these predictions in the context of our original data, we need to rescale them back to the original range. This is achieved by **reversing the normalization process** ((pred_df * (train_max - train_min)) + train_min), converting our predictions to the same scale as the original PUN values.","metadata":{}},{"cell_type":"code","source":"# create train validation dataset\nstart = past \n\nX_train_val = train_data_norm.values\ny_train_val = train_data_norm[['pun']].iloc[start:].values\n\n\n# create test  dataset\nstart = past \n\nX_test = test_data_norm.values\ny_test = test_data_norm[['pun']].iloc[start:].values\n\n\n\ndataset_train_val =  tf.keras.preprocessing.timeseries_dataset_from_array(\n    X_train_val,\n    y_train_val,\n    sequence_length=sequence_length,\n    batch_size=batch_size,\n   \n)\n\ndataset_test =  tf.keras.preprocessing.timeseries_dataset_from_array(\n    X_test,\n    y_test,\n    sequence_length=sequence_length,\n    batch_size=batch_size,\n   \n)","metadata":{"id":"9zMwvjP4I0mk","execution":{"iopub.status.busy":"2023-07-24T11:34:02.401359Z","iopub.execute_input":"2023-07-24T11:34:02.40238Z","iopub.status.idle":"2023-07-24T11:34:02.528879Z","shell.execute_reply.started":"2023-07-24T11:34:02.402335Z","shell.execute_reply":"2023-07-24T11:34:02.527765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(dataset_train_val)","metadata":{"id":"sEulYygYHxnP","outputId":"59c3e1db-fb73-46e1-97dd-706cb846d427","execution":{"iopub.status.busy":"2023-07-24T11:34:02.530497Z","iopub.execute_input":"2023-07-24T11:34:02.53087Z","iopub.status.idle":"2023-07-24T11:35:00.566456Z","shell.execute_reply.started":"2023-07-24T11:34:02.53083Z","shell.execute_reply":"2023-07-24T11:35:00.565244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"id":"MCqv0n_gJAzi","outputId":"be794c8a-efec-44ed-af29-4e96b1d0578d","execution":{"iopub.status.busy":"2023-07-24T11:35:00.56836Z","iopub.execute_input":"2023-07-24T11:35:00.568797Z","iopub.status.idle":"2023-07-24T11:35:02.14248Z","shell.execute_reply.started":"2023-07-24T11:35:00.568755Z","shell.execute_reply":"2023-07-24T11:35:02.141343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame(y_pred, columns=['pun'])","metadata":{"id":"DIzweIJUPywt","execution":{"iopub.status.busy":"2023-07-24T11:35:02.143996Z","iopub.execute_input":"2023-07-24T11:35:02.144683Z","iopub.status.idle":"2023-07-24T11:35:02.150168Z","shell.execute_reply.started":"2023-07-24T11:35:02.144649Z","shell.execute_reply":"2023-07-24T11:35:02.14879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = (pred_df *  (train_max - train_min))  + train_min \npred_df","metadata":{"id":"qf4ahmKdOkJs","outputId":"5c058da1-c557-4a32-bcb0-84e2d53c886d","execution":{"iopub.status.busy":"2023-07-24T11:35:02.151752Z","iopub.execute_input":"2023-07-24T11:35:02.152497Z","iopub.status.idle":"2023-07-24T11:35:02.171883Z","shell.execute_reply.started":"2023-07-24T11:35:02.152457Z","shell.execute_reply":"2023-07-24T11:35:02.170875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:35:02.173367Z","iopub.execute_input":"2023-07-24T11:35:02.173825Z","iopub.status.idle":"2023-07-24T11:35:02.186234Z","shell.execute_reply.started":"2023-07-24T11:35:02.173786Z","shell.execute_reply":"2023-07-24T11:35:02.184896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#last_period = split_size_test - pred_hours\nplot_two_lines( 0, test_data.iloc[-len(test_data):].index,pred_df.iloc[-len(test_data):]['pun'],  test_data.iloc[-len(test_data):].index, test_data['pun'].iloc[-len(test_data):], \\\n               ylabel=\"Pun\",data1_label= 'Train Data', data2_label = 'Test Data' ,\n               title=\"Slice of Train and Test Data over Time\", \n               color1 = '#001a4d', color2 = \"#ff7f50\"\n               )","metadata":{"id":"sLBUapWJQ56x","outputId":"483a0696-ac54-4689-c633-6ca0901cb820","execution":{"iopub.status.busy":"2023-07-24T11:35:02.188057Z","iopub.execute_input":"2023-07-24T11:35:02.188803Z","iopub.status.idle":"2023-07-24T11:35:02.552325Z","shell.execute_reply.started":"2023-07-24T11:35:02.188765Z","shell.execute_reply":"2023-07-24T11:35:02.551258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_metrics (test_data['pun'],pred_df['pun'])","metadata":{"id":"uv9IYhILT4Al","outputId":"73e8a46f-b394-42e0-c182-e10608621bdf","execution":{"iopub.status.busy":"2023-07-24T11:35:02.553953Z","iopub.execute_input":"2023-07-24T11:35:02.554683Z","iopub.status.idle":"2023-07-24T11:35:02.56738Z","shell.execute_reply.started":"2023-07-24T11:35:02.554644Z","shell.execute_reply":"2023-07-24T11:35:02.566041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, various error metrics (R2 score, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Median Absolute Error (MdAE) and Mean Percentage Error (MAPE)) are calculated using the error_metrics function. These metrics provide us with different ways to understand the performance of our model. For instance, the R2 score (the coefficient of determination) tells us the proportion of the variance in the dependent variable that is predictable from the independent variable(s). MSE, RMSE, MAE, MdAE provide measures of the differences between values predicted by the model and the values actually observed. The MAPE gives us a percentage error, helping to understand the accuracy of the model in relative terms.\n\n**The LSTM model performance seems to be substantially better than the baseline model, as the errors are significantly lower in all metrics, and the R2 score is higher.**","metadata":{}}]}