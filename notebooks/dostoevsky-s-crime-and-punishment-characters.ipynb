{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gianpieroandrenacci/dostoevsky-s-crime-and-punishment-characters?scriptVersionId=154888545\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 📕 Identify Dostoevsky's Crime and Punishment characters\n\n<blockquote style=\"color:#33001a; font-family:cursive;font-size:130%\">I would like to live in a world where science and humanitas are just one thing</blockquote>","metadata":{}},{"cell_type":"markdown","source":"![Crime_And_Punishment.png](https://www.npensieri.it/wp-content/uploads/Delitto_E_Castigo.png)","metadata":{}},{"cell_type":"markdown","source":"📑  \n* [1. Introduction](#1)\n    * [1.1 Import Libraries](#1.1)\n    * [1.2 Custom Functions](#1.2)\n    * [1.3 Spacy Library](#1.3)\n    * [1.4 Import the book](#1.4)\n* [2. Characters Identification](#2)\n    * [2.1 Clean the text](#2.1)\n    * [2.2 Entitity Recognition](#2.2)\n    * [2.3 Characters Entitity Recognition](#2.3)\n    * [2.4 Improve Characters Identification](#2.4)\n    * [2.5 Name Variation Resolution](#2.5)\n    * [2.6 Final Word Cloud](#2.6)\n    * [2.7 Reference](#2.7!)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Introduction</h1>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert alert-danger\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em; \">\n    📌 &nbsp; Please keep in mind that, we are always on our learning journey. I just want to share something that will be interesting for you. If you find this notebook useful in anyway, please upvote it so that it can reach a bigger audience. You can share it with your fellow kagglers.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:130%; background-color:#f28482;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">\"Crime and Punishment\" by Fyodor Dostoevsky is a classic novel that features a wide range of characters. Here are the top 20 characters in the book:</p>\n\n1. Rodion Romanovich Raskolnikov - The main protagonist of the novel who murders an old pawnbroker and her sister.\n2. Avdotya Romanovna Raskolnikova (Dounia) - Raskolnikov's sister who is engaged to Luzhin.\n3. Arkady Ivanovich Svidrigailov - A wealthy, powerful and mysterious man who is interested in Raskolnikov's sister Dounia.\n4. Porfiry Petrovich - A shrewd police detective who is investigating the murders.\n5. Sofya Semyonovna Marmeladova (Sonya) - A young prostitute who is forced into the trade to support her family.\n6. Dmitri Prokofych Razumikhin - Raskolnikov's loyal and compassionate friend.\n7. Pulcheria Alexandrovna Raskolnikova - Raskolnikov's mother who comes to visit him in St. Petersburg.\n8. Luzhin - Dounia's fiance who is selfish and manipulative.\n9. Katerina Ivanovna Marmeladova - Sonya's stepmother who is consumed by her own pride and vanity.\n10. Nastasya Petrovna - A troubled woman who works as a servant for Svidrigailov.\n11. Ilya Petrovich - A corrupt and abusive police officer.\n12. Zossimov - A doctor who treats Raskolnikov after his illness.\n13. Nikolai Dementiev - A painter who is wrongly accused of the murders.\n14. Pyotr Petrovich Luzhin - Luzhin's father who is a high-ranking government official.\n15. Vasiliev - A clerk who works for Svidrigailov.\n16. Andrey Semyonovich Lebezyatnikov - A boarder who lives in the same building as Raskolnikov and is interested in socialist theories.\n17. Lizaveta Ivanovna - The pawnbroker's sister who is also murdered by Raskolnikov.\n18. Zametov - A young police officer who assists Porfiry Petrovich in the investigation.\n19. Praskovya Pavlovna - The landlady of the building where Raskolnikov lives.\n20. Zamyotov - Another police officer who is also involved in the investigation.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">1.1 Import libraries</h1>","metadata":{}},{"cell_type":"code","source":"!pip install textacy\n!pip install tqdm","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-10T11:50:17.429153Z","iopub.execute_input":"2023-03-10T11:50:17.429646Z","iopub.status.idle":"2023-03-10T11:50:41.44423Z","shell.execute_reply.started":"2023-03-10T11:50:17.4296Z","shell.execute_reply":"2023-03-10T11:50:41.443001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport regex as re\nfrom wordcloud import WordCloud \nfrom collections import Counter \n\nfrom tqdm.auto import tqdm\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport html\n\nimport spacy\nfrom spacy.tokens import Span\nfrom spacy import Language\nfrom spacy import displacy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n\n\n# charachet normalization with textacy\nimport textacy\nimport textacy.preprocessing as tprep\nimport nltk\n\n\nimport requests\nimport io\nimport re\nimport random","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-03-10T11:50:41.447652Z","iopub.execute_input":"2023-03-10T11:50:41.448075Z","iopub.status.idle":"2023-03-10T11:50:41.458859Z","shell.execute_reply.started":"2023-03-10T11:50:41.448034Z","shell.execute_reply":"2023-03-10T11:50:41.457562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_core_web_md","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-03-10T11:50:41.460004Z","iopub.execute_input":"2023-03-10T11:50:41.460318Z","iopub.status.idle":"2023-03-10T11:51:08.832996Z","shell.execute_reply.started":"2023-03-10T11:50:41.460286Z","shell.execute_reply":"2023-03-10T11:51:08.831911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    \"\"\"\n    A function that takes a string of text as input and applies several regular expression patterns to remove unwanted \n    characters or patterns from the text.\n\n    Args:\n    - text: A string of text to be cleaned.\n\n    Returns:\n    - A cleaned version of the input text with unwanted characters or patterns removed.\n    \"\"\"\n  \n    text = re.sub(r'<[^<>]*>', ' ', text)  # removes HTML tags\n    \n    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)  # removes markdown links\n    \n    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)  # removes markdown reference links\n   \n    text = re.sub(r'(\"|_)', ' ', text)  # removes quotes and underscores\n    \n    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)  # removes special characters and symbols\n    \n    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)  # removes consecutive equal signs or dashes\n    \n    text = re.sub(r'\\s+', ' ', text)  # replaces multiple white spaces with a single space\n   \n    text = re.sub(r'\\--+', ' ', text)  # replaces multiple dashes with a single space\n    \n    return text.strip()  # removes any leading or trailing white spaces from the cleaned text\n\n\ndef normalize(text):\n    \"\"\"\n    A function that takes a string of text as input and applies several normalization techniques to standardize \n    the text for further processing or analysis.\n\n    Args:\n    - text: A string of text to be normalized.\n\n    Returns:\n    - A normalized version of the input text.\n    \"\"\"\n\n    text = tprep.normalize.hyphenated_words(text)  # replaces hyphenated words with a single word\n    text = tprep.normalize.quotation_marks(text)  # replaces different types of quotation marks with standard quotation marks\n    text = tprep.normalize.unicode(text)  # replaces unicode characters with their ASCII equivalents\n    text = tprep.remove.accents(text)  # removes diacritical marks from accented characters\n    return text   \n\n\ndef count_words(df, column='tokens', preprocess=None, min_freq=2):\n    \"\"\"\n    A function that takes a pandas DataFrame, a column name, a preprocess function, and a minimum frequency count as input, \n    and returns a sorted DataFrame of the most frequently occurring words in the specified column.\n\n    Args:\n    - df: A pandas DataFrame containing the text data.\n    - column: A string representing the column name that contains the text data to be analyzed. Defaults to 'tokens'.\n    - preprocess: A function that preprocesses the text data before counting the words. Defaults to None.\n    - min_freq: An integer representing the minimum frequency count of a word to be included in the output. Defaults to 2.\n\n    Returns:\n    - A pandas DataFrame containing the most frequently occurring words in the specified column, sorted in descending order \n      of frequency.\n    \"\"\"\n    \n    # process tokens and update counter\n    def update(doc):\n        \"\"\"\n        A helper function that takes a document (a string) as input, preprocesses the document if a preprocess function is \n        provided, and updates the word counter with the resulting tokens.\n\n        Args:\n        - doc: A string representing a document to be processed.\n\n        Returns:\n        - None\n        \"\"\"\n        tokens = doc if preprocess is None else preprocess(doc)\n        counter.update(tokens)\n\n    # create counter and run through all data\n    counter = Counter()\n    df[column].map(update)\n\n    # transform counter into data frame\n    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n    freq_df = freq_df.query('freq >= @min_freq')\n    freq_df.index.name = 'token'\n\n    return freq_df.sort_values('freq', ascending=False)\n\n\ndef wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n    \"\"\"\n    Create a word cloud visualization from a frequency counter.\n\n    Parameters:\n    ----------\n    word_freq : dict or pd.Series\n        A dictionary or pandas Series containing the word frequency data.\n    title : str, optional\n        The title of the word cloud plot. Default is None.\n    max_words : int, optional\n        The maximum number of words to be displayed in the word cloud plot. Default is 200.\n    stopwords : set or None, optional\n        A set of stop words to be filtered out from the word frequency data. Default is None.\n\n    Returns:\n    --------\n    None, displays the plot\n\n    Example: wordcloud(word_freq, 'Fruit Word Cloud', max_words=50, stopwords=stopwords)\n    --------\n    \"\"\"\n\n    # Create an instance of the WordCloud class with specific parameters.\n    wc = WordCloud(width=800, height=400, \n                   background_color= \"White\", colormap=\"inferno\", \n                   max_font_size=150, max_words=max_words)\n    \n    # Convert input word frequency data into a frequency counter.\n    if type(word_freq) == pd.Series:\n        counter = Counter(word_freq.fillna(0).to_dict())\n    else:\n        counter = word_freq\n\n    # Filter out stop words from the frequency counter.\n    if stopwords is not None:\n        counter = {token:freq for (token, freq) in counter.items() \n                              if token not in stopwords}\n\n    # Generate the word cloud visualization from the frequency counter.\n    wc.generate_from_frequencies(counter)\n \n    # Display the word cloud visualization with an optional title.\n    plt.title(title) \n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n        \n\ndef extract_entities(text, include_types=None, sep=' '):\n\n    \"\"\"The function extracts named entities from a given text using spaCy and textacy libraries\n    It returns a list of extracted entities after applying lemmatization and joining the \n    tokens with the specified separator\n    Parameters:\n    -----------\n    text : str\n    Input text from which named entities are to be extracted.\n    include_types : list, optional\n    List of entity types to include in the extraction. If not provided, all types will be included.\n    sep : str, optional\n    Separator to join the tokens of each named entity.\n\n    Returns:\n    --------\n    entity_list : list\n    A list of extracted named entities after applying lemmatization and joining the tokens with the specified separator.\n\n    \"\"\"\n\n    # define the doc\n    doc = nlp(text)\n\n    # extract the entities\n    ents = textacy.extract.entities(doc, \n    include_types=include_types, \n    exclude_types=None, \n    drop_determiners=True, \n    min_freq=1)\n\n    # build the entity list\n    entity_list = [sep.join([t.lemma_ for t in e]) for e in ents]\n    return entity_list\n\n\n\n\ndef plot_bar_custom(df_toplot, desc,column,color1 = '#660066', color2 = '#33001a'):\n    \"\"\"\n    Nice bar plot function that creates a horizontal bar plot \n    with dots for a given dataframe and column, \n    setting custom colors and style for the plot\n\n    Parameters:\n    -----------\n    df : pandas DataFrame\n    The DataFrame containing the data to plot\n    desc : str\n    The description or title for the plot\n    column : str\n    The column to use for plotting\n    color1 : str, optional (default='#660066')\n    The color to use for the bars and dots\n    color2 : str, optional (default='#33001a')\n    The color to use for the axes and text\n\n    Returns:\n    --------\n    None, displays the plot\n\n    \"\"\" \n\n    # set the style of the axes and the text color\n    plt.rcParams['axes.edgecolor']=color2\n    plt.rcParams['axes.linewidth']=0.8\n    plt.rcParams['xtick.color']=color2\n    plt.rcParams['ytick.color']=color2\n    plt.rcParams['text.color']=color2\n\n\n    # we first need a numeric placeholder for the y axis\n    my_range=list(range(1,len(df_toplot.index)+1))\n\n    fig, ax = plt.subplots(figsize=(10,5.5))\n\n    ax.yaxis.grid(False, which='major')\n\n    df_toplot = df_toplot.sort_values(by=column)\n    # create for each expense type an horizontal line that starts at x = 0 with the length \n    # represented by the specific expense percentage value.\n    plt.hlines(y=my_range, xmin=0, xmax=df_toplot[column], color=color1, alpha=0.5, linewidth=5)\n\n    # create for each expense type a dot at the level of the expense percentage value\n    plt.plot(df_toplot[column], my_range, \"o\", markersize=5, color=color1, alpha=1)\n\n    # set labels\n    ax.set_xlabel(column, fontsize=15, fontweight='black', color =color2)\n    ax.set_ylabel('')\n\n    # set axis\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    plt.yticks(my_range, df_toplot.index)\n\n    # add an horizonal label for the y axis \n    fig.text(-0.23, 0.96, desc, fontsize=15, fontweight='black', color = color2)\n\n    # change the style of the axis spines\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax.spines['left'].set_bounds((1, len(my_range)))\n    ax.set_xlim(0,80)\n\n    ax.spines['left'].set_position(('outward', 8))\n    ax.spines['bottom'].set_position(('outward', 5))\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-10T11:51:08.83777Z","iopub.execute_input":"2023-03-10T11:51:08.838243Z","iopub.status.idle":"2023-03-10T11:51:08.872226Z","shell.execute_reply.started":"2023-03-10T11:51:08.838196Z","shell.execute_reply":"2023-03-10T11:51:08.870262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">SpacyLibrary</h1>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#600080;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\"> SpaCy is an open-source natural language processing (NLP) library that provides a wide range of features for processing text. Its pipeline consists of several steps, each of which is responsible for a specific task.\n</div>","metadata":{}},{"cell_type":"code","source":"# check spacy version\nassert spacy.__version__[0] >= '3'","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:08.874197Z","iopub.execute_input":"2023-03-10T11:51:08.874678Z","iopub.status.idle":"2023-03-10T11:51:08.893588Z","shell.execute_reply.started":"2023-03-10T11:51:08.874624Z","shell.execute_reply":"2023-03-10T11:51:08.892396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_md\")\n# not in book: make sure stop words are available\nnltk.download('stopwords')\n\nstopwords = set(nltk.corpus.stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:08.894975Z","iopub.execute_input":"2023-03-10T11:51:08.895591Z","iopub.status.idle":"2023-03-10T11:51:10.972634Z","shell.execute_reply.started":"2023-03-10T11:51:08.89554Z","shell.execute_reply":"2023-03-10T11:51:10.971256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#600080;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\"> The en_core_web_md model in spaCy is an English language pipeline optimized for running on a CPU. It consists of the following components:\n</div>\n\n1. **Tokenizer** (tok2vec): The tokenizer component is responsible for breaking up the input text into individual tokens or words. In the en_core_web_md model, the tokenizer is implemented using a neural network architecture called tok2vec.\n\n2. **Part-of-speech Tagger** (tagger): This component assigns each token a part-of-speech tag, such as noun, verb, or adjective. The tagger in en_core_web_md uses a convolutional neural network to predict the POS tags.\n\n3. **Dependency Parser** (parser): The parser analyzes the grammatical structure of the sentence and identifies the relationships between the words. The parser in en_core_web_md is based on a neural network architecture called a transition-based parser.\n\n4. **Sentence Boundary Detector** (senter): The sentence boundary detector identifies the boundaries between sentences in the input text. In en_core_web_md, this is implemented using a rule-based approach.\n\n5. **Named Entity Recognizer** (ner): The NER component identifies and categorizes named entities in the input text, such as people, organizations, and locations. The NER in en_core_web_md uses a neural network architecture called a linear-chain conditional random field.\n\n6. **Attribute Ruler** (attribute_ruler): This component allows for the manipulation of token attributes, such as adding new attributes or modifying existing ones. In en_core_web_md, the attribute ruler is used to add lemma and stop word attributes.\n\n7. **Lemmatizer** (lemmatizer): The lemmatizer maps each token to its base form, or lemma. In en_core_web_md, the lemmatizer is implemented using a rule-based approach.\n\nOverall, the en_core_web_md model provides a powerful suite of NLP tools that can be used for a variety of tasks, from simple tokenization to complex text analysis. It is optimized for running on a CPU, making it a good choice for applications where CPU resources are limited.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.4\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Import the book</h1>","metadata":{}},{"cell_type":"markdown","source":"<table class=\"bibrec\">\n<colgroup>\n<col class=\"narrow\">\n<col>\n</colgroup>\n<tr>\n<th>Author</th>\n<td>\n<a href=\"https://www.gutenberg.org/ebooks/author/314\" rel=\"marcrel:aut\" about=\"/authors/314\" typeof=\"pgterms:agent\" itemprop=\"creator\">Dostoyevsky, Fyodor, 1821-1881</a></td>\n</tr><tr>\n<th>Translator</th>\n<td>\n<a href=\"https://www.gutenberg.org/ebooks/author/2858\" rel=\"marcrel:trl\" about=\"/authors/2858\" typeof=\"pgterms:agent\" itemprop=\"creator\">Garnett, Constance, 1861-1946</a></td>\n</tr>\n<tr>\n<th>Uniform Title</th>\n<td itemprop=\"alternativeHeadline\">\nPrestuplenie i nakazanie. English\n</td>\n</tr><tr>\n<th>Title</th>\n<td itemprop=\"headline\">\nCrime and Punishment\n</td>\n</tr>\n<tr property=\"dcterms:language\" datatype=\"dcterms:RFC4646\" itemprop=\"inLanguage\" content=\"en\">\n<th>Language</th>\n<td>Inglese</td>\n</tr>\n<tr property=\"dcterms:subject\" datatype=\"dcterms:LCC\" content=\"PG\">\n<th>LoC Class</th>\n<td>\n<a href=\"/browse/loccs/pg\">PG: Language and Literatures: Slavic (including Russian), Languages and Literature</a>\n</td>\n</tr>\n<tr>\n<th>Subject</th>\n<td property=\"dcterms:subject\" datatype=\"dcterms:LCSH\">\n<a class=\"block\" href=\"https://www.gutenberg.org/ebooks/subject/1123\">\nDetective and mystery stories\n</a>\n</td>\n</tr><tr>\n<th>Subject</th>\n<td property=\"dcterms:subject\" datatype=\"dcterms:LCSH\">\n<a class=\"block\" href=\"https://www.gutenberg.org/ebooks/subject/2430\">\nPsychological fiction\n</a>\n</td>\n</tr><tr>\n<th>Subject</th>\n<td property=\"dcterms:subject\" datatype=\"dcterms:LCSH\">\n<a class=\"block\" href=\"https://www.gutenberg.org/ebooks/subject/3523\">\nSaint Petersburg (Russia) -- Fiction\n</a>\n</td>\n</tr><tr>\n<th>Subject</th>\n<td property=\"dcterms:subject\" datatype=\"dcterms:LCSH\">\n<a class=\"block\" href=\"https://www.gutenberg.org/ebooks/subject/3550\">\nMurder -- Fiction\n</a>\n</td>\n</tr><tr>\n<th>Subject</th>\n<td property=\"dcterms:subject\" datatype=\"dcterms:LCSH\">\n<a class=\"block\" href=\"https://www.gutenberg.orghttps://www.gutenberg.org/ebooks/subject/34776\">\nCrime -- Psychological aspects -- Fiction\n</a>\n</td>\n</tr>\n<tr>\n<th>Category</th>\n<td property=\"dcterms:type\" datatype=\"dcterms:DCMIType\">Text</td>\n</tr>\n<tr>\n<th>EBook-No.</th>\n<td>2554</td>\n</tr>\n<tr property=\"dcterms:issued\" datatype=\"xsd:date\" content=\"2006-03-28T00:00:00+00:00\">\n<th>Release Date</th>\n<td itemprop=\"datePublished\">28 mar 2006</td>\n</tr>\n<tr>\n<th>Copyright Status</th>\n<td property=\"dcterms:rights\">Public domain in the USA.</td>\n</tr>\n<tr>\n<th>Downloads</th>\n<td itemprop=\"interactionCount\">14065 downloads in the last 30 days.</td>\n</tr>\n<tr itemprop=\"offers\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Offer\">\n<td colspan=\"2\"><span itemprop=\"priceCurrency\" content=\"USD\"></span><span itemprop=\"price\" content=\"$0.00\"><em>Project Gutenberg books are always free!</em></span><span itemprop=\"availability\" content=\"In Stock\"><a href=\"http://schema.org/InStock\"></a></span></td>\n</tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#600080;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 10px;\n              color:white;\"> Download the book From Gutemberg\n</div>","metadata":{}},{"cell_type":"code","source":"# download the book from Gutenberg and store it in a text variable. \n\nurl = \"https://www.gutenberg.org/files/2554/2554-0.txt\"\nresponse = requests.get(url)\nbook = str(response.content,encoding='utf8' )","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:10.974565Z","iopub.execute_input":"2023-03-10T11:51:10.975069Z","iopub.status.idle":"2023-03-10T11:51:11.628527Z","shell.execute_reply.started":"2023-03-10T11:51:10.975018Z","shell.execute_reply":"2023-03-10T11:51:11.627355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(book[:3000])","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:11.630384Z","iopub.execute_input":"2023-03-10T11:51:11.630972Z","iopub.status.idle":"2023-03-10T11:51:11.638666Z","shell.execute_reply.started":"2023-03-10T11:51:11.630829Z","shell.execute_reply":"2023-03-10T11:51:11.637223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the preface\nsubstring = \"became great.”\"\nposition = book.find(substring)\nposition = position + len(substring) - 1\nbook = book[position:]\nprint(book[:1000])","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:11.640167Z","iopub.execute_input":"2023-03-10T11:51:11.640651Z","iopub.status.idle":"2023-03-10T11:51:11.656534Z","shell.execute_reply.started":"2023-03-10T11:51:11.640612Z","shell.execute_reply":"2023-03-10T11:51:11.65507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:200%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Characters Identification</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Clean the Text</h1>","metadata":{}},{"cell_type":"code","source":"# save the book in a dataframe\ndf_book = pd.DataFrame(columns=['text','clean_text'])\ndf_book.loc[0, 'text'] = book","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:11.662564Z","iopub.execute_input":"2023-03-10T11:51:11.662935Z","iopub.status.idle":"2023-03-10T11:51:11.676453Z","shell.execute_reply.started":"2023-03-10T11:51:11.662901Z","shell.execute_reply":"2023-03-10T11:51:11.675258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_book['clean_text'] = df_book['text'].progress_map(clean)\ndf_book['clean_text'] = df_book['clean_text'].progress_map(normalize)\ndf_book.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\ndf_book.drop(columns = ['raw_text'], inplace = True)\nprint(df_book['text'])","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:11.678056Z","iopub.execute_input":"2023-03-10T11:51:11.678417Z","iopub.status.idle":"2023-03-10T11:51:12.467993Z","shell.execute_reply.started":"2023-03-10T11:51:11.678385Z","shell.execute_reply":"2023-03-10T11:51:12.466677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Named Entity Recognition</h1>","metadata":{}},{"cell_type":"markdown","source":"**Named Entity Recognition (NER)** is a subfield of Natural Language Processing (NLP) that focuses on identifying and categorizing named entities in text. Named entities are typically defined as words or phrases that refer to specific individuals, organizations, locations, products, or other types of entities that have a proper name.\n\nNER is an important technique in many NLP applications, such as information retrieval, information extraction, **sentiment analysis**, and text classification. By identifying and categorizing named entities in text, NER systems can help to improve the accuracy of these applications.\n\nThere are several approaches to NER, including **rule-based systems**, statistical models, and deep learning techniques. Rule-based systems rely on manually defined rules to identify and categorize named entities in text. Statistical models use machine learning algorithms to learn patterns in text that correspond to named entities. Deep learning techniques, such as neural networks, can automatically learn representations of text that are useful for identifying named entities.\n\nOne of the key challenges in NER is dealing with **ambiguous entities**. For example, the name \"Amazon\" could refer to the online retailer, the rainforest, or the river. NER systems must be able to disambiguate these entities based on the context in which they appear.\n","metadata":{}},{"cell_type":"code","source":"text = \"Raskolnikov recognised Katerina Ivanovna at once. She was a rather tall, \\\nslim and graceful woman, terribly emaciated, with magnificent dark brown \\\nhair and with a hectic flush in her cheeks.\"\ndoc = nlp(text)\ndisplacy.render(doc, style='ent', jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:12.469851Z","iopub.execute_input":"2023-03-10T11:51:12.47033Z","iopub.status.idle":"2023-03-10T11:51:12.502772Z","shell.execute_reply.started":"2023-03-10T11:51:12.470283Z","shell.execute_reply":"2023-03-10T11:51:12.501448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Characters Identification</h1>","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=['tokens','num_tokens'])\n\n# This sets the maximum length of the spaCy language model to 1155653 characters. \n# to avoid memory errors when processing large amounts of text.\nnlp.max_length = 1155653 \n\n# Calculates the frequency of occurrence of each word in the \n# \"tokens\" column of the DataFrame and returns a new DataFrame with two \"frequency\"\n\ndf['tokens'] = df_book['text'].apply(extract_entities,args=(['PERSON']))\ndf['num_tokens'] = df['tokens'].map(len)\nfreq_df = count_words(df)\nfreq_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:51:12.50451Z","iopub.execute_input":"2023-03-10T11:51:12.504877Z","iopub.status.idle":"2023-03-10T11:52:01.005995Z","shell.execute_reply.started":"2023-03-10T11:51:12.50484Z","shell.execute_reply":"2023-03-10T11:52:01.004597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WordCloud\nplt.figure(figsize=(16,8)) ###\nfreq_df = freq_df.apply(np.sqrt)\nwordcloud(freq_df['freq'], title='Characters',max_words=50)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.008335Z","iopub.execute_input":"2023-03-10T11:52:01.009217Z","iopub.status.idle":"2023-03-10T11:52:01.693503Z","shell.execute_reply.started":"2023-03-10T11:52:01.009158Z","shell.execute_reply":"2023-03-10T11:52:01.692138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Improve Characters Identification</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:130%; background-color:#f28482;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Normalizing</p>","metadata":{}},{"cell_type":"markdown","source":"**Article and  and  an\napostrophe-s**\n​\nThe entity detection process can sometimes detect tokens that are not part of the actual entity name, such as in the case of \"Katerina Ivanovna's\". \n​\nThe same issue occurs with tokens like \"the\" in \"the Italian Stock Exchange\", which may or may not be considered part of the entity name. To simplify the linking of entity mentions, it is often helpful to remove these types of tokens, including articles and apostrophe-s, from the entity names. \n​\nHowever, it's important to note that such rules may not always be accurate, as seen with entities like \"The avengers\" or \"McDonald's\", where preserving the article or apostrophe-s is necessary. In these cases, exceptions to the rules must be defined to avoid errors in entity linking.","metadata":{}},{"cell_type":"code","source":"test_string = 'Katerina Ivanovna \\'s'\ndoc = nlp(test_string)\ndisplacy.render(doc, style='ent', jupyter=True)\nprint(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.695499Z","iopub.execute_input":"2023-03-10T11:52:01.696253Z","iopub.status.idle":"2023-03-10T11:52:01.715141Z","shell.execute_reply.started":"2023-03-10T11:52:01.696211Z","shell.execute_reply":"2023-03-10T11:52:01.714036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@Language.component(\"norm_entities\")\ndef norm_entities(doc):\n    \"\"\"\n    A custom spaCy component to normalize entities in a document by removing leading\n    articles and trailing particles like 's.\n\n    Args:\n        doc (spacy.tokens.Doc): The input spaCy Doc object.\n\n    Returns:\n        spacy.tokens.Doc: The modified spaCy Doc object with normalized entities.\n    \"\"\"\n    # function code here\n\n    ents = []\n    for ent in doc.ents:\n        if ent[0].pos_ == \"DET\": # leading article\n            ent = Span(doc, ent.start+1, ent.end, label=ent.label)\n        if len(ent) > 0:\n            if ent[-1].pos_ == \"PART\": # trailing particle like 's\n                ent = Span(doc, ent.start, ent.end-1, label=ent.label)\n            ents.append(ent)\n    doc.ents = tuple(ents)\n    return doc ","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.716438Z","iopub.execute_input":"2023-03-10T11:52:01.716824Z","iopub.status.idle":"2023-03-10T11:52:01.730424Z","shell.execute_reply.started":"2023-03-10T11:52:01.716779Z","shell.execute_reply":"2023-03-10T11:52:01.728866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the pipeline norm_entities\n\nif nlp.has_pipe('norm_entities'): \n    _ = nlp.remove_pipe('norm_entities') \nnlp.add_pipe('norm_entities')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.732545Z","iopub.execute_input":"2023-03-10T11:52:01.733042Z","iopub.status.idle":"2023-03-10T11:52:01.751932Z","shell.execute_reply.started":"2023-03-10T11:52:01.732999Z","shell.execute_reply":"2023-03-10T11:52:01.750584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of norm_entities\ntest_string = 'Katerina Ivanovna \\'s'\ndoc = nlp(test_string)\ndisplacy.render(doc, style='ent', jupyter=True)\nprint(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.754619Z","iopub.execute_input":"2023-03-10T11:52:01.755347Z","iopub.status.idle":"2023-03-10T11:52:01.778126Z","shell.execute_reply.started":"2023-03-10T11:52:01.755293Z","shell.execute_reply":"2023-03-10T11:52:01.776641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:130%; background-color:#f28482;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Merge Entities</p>","metadata":{}},{"cell_type":"markdown","source":"**Compound names** like \"Katerina Ivanovna\" like\nthe previous example is trated as single tokens because it simplifies the sentence structure. spaCy pro‐vides a built-in pipeline function  **merge_entities**  for that purpose. We add it to our\nNLP pipeline and get exactly one token per named-entity.","metadata":{}},{"cell_type":"code","source":"from spacy.pipeline import merge_entities\n\n# add the pipeline merge_entities\n\nif nlp.has_pipe('merge_entities'): \n    _ = nlp.remove_pipe('merge_entities') \nnlp.add_pipe('merge_entities')\n\ndoc = nlp(text)\nprint(*[(t.text, t.ent_type_) for t in doc if t.ent_type_ != ''])","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.779643Z","iopub.execute_input":"2023-03-10T11:52:01.780492Z","iopub.status.idle":"2023-03-10T11:52:01.902976Z","shell.execute_reply.started":"2023-03-10T11:52:01.780421Z","shell.execute_reply":"2023-03-10T11:52:01.901485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:130%; background-color:#f28482;border-radius: 0px 30px;text-align:center;font-family:Sans-serif;font-weight:bold\">Repeat the entity recognition with new pipelines step</p>","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=['tokens','num_tokens'])\n\n# calculates the frequency of occurrence of each word in the \"tokens\" \n# column of the DataFrame and returns a new DataFrame with frequency\n\ndf['tokens'] = df_book ['text'].apply(extract_entities,args=(['PERSON']))\ndf['num_tokens'] = df['tokens'].map(len)\nfreq_df = count_words(df)\nfreq_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:01.90551Z","iopub.execute_input":"2023-03-10T11:52:01.906005Z","iopub.status.idle":"2023-03-10T11:52:49.534797Z","shell.execute_reply.started":"2023-03-10T11:52:01.905956Z","shell.execute_reply":"2023-03-10T11:52:49.533452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8)) ###\nfreq_df = freq_df.apply(np.sqrt)\nwordcloud(freq_df['freq'], title='Characters',max_words=50)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:49.536705Z","iopub.execute_input":"2023-03-10T11:52:49.537698Z","iopub.status.idle":"2023-03-10T11:52:50.209725Z","shell.execute_reply.started":"2023-03-10T11:52:49.537641Z","shell.execute_reply":"2023-03-10T11:52:50.208562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.5\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Name Variation Resolution</h1","metadata":{}},{"cell_type":"markdown","source":"**Name Variation Resolution** is the process of identifying and merging different variations or representations of the same entity name into a single, canonical form. This is a common problem in natural language processing, especially in tasks such as named entity recognition and entity linking, where it is important to **correctly identify and link all mentions of the same entity** across different documents or contexts. \n\n**Name variations can include different spellings, abbreviations, aliases, titles, honorifics**, etc. Effective name variation resolution involves both rule-based and machine learning approaches, and often involves the use of external knowledge sources such as dictionaries, gazetteers, and knowledge graphs.\n\n**We will adpt a simple rule-based approch.**","metadata":{}},{"cell_type":"code","source":"# Find all the name that have common words\n\n# Tokenize the index values into sets of words\ntokenized_index = [set(x.split()) for x in freq_df.index]\n\n# Find index entries that have words in common\ncommon_word_pairs = []\nfor i, index_i in enumerate(tokenized_index):\n    for j, index_j in enumerate(tokenized_index[i+1:], start=i+1):\n        common_words = index_i.intersection(index_j)\n        if common_words:\n            common_word_pairs.append((freq_df.index[i], freq_df.index[j], common_words))\n\n# Print the pairs of index values and the common words between them\nfor pair in common_word_pairs:\n    print(f\"{pair[0]} and {pair[1]} have these words in common: {pair[2]}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.211315Z","iopub.execute_input":"2023-03-10T11:52:50.212484Z","iopub.status.idle":"2023-03-10T11:52:50.226487Z","shell.execute_reply.started":"2023-03-10T11:52:50.212394Z","shell.execute_reply":"2023-03-10T11:52:50.225002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the my_rules dictionary values in the dataframe freq_df index\n# replace the dataframe freq_df index found with the corresponding my_rules dictionary key\n\nfreq_df = freq_df.copy()\n\n# Define the dictionary rules\n\nmy_rules = {\n    'Afanasy Ivanovitch Vahrushin': ['Afanasy Ivanovitch' ],\n    'Avdotya Romanovna':['Dounia'],\n    'Dmitri Prokofitch':['Dmitri'],\n    'Pyotr Petrovitch Luzhin':['Pyotr Petrovitch','Luzhin'],\n    'Rodion Romanovitch Raskolnikov':['Rodion Romanovitch', 'Raskolnikov'],\n    'Sofya Semyonovna Marmeladov':['Sofya Semyonovna'],\n    'Amalia Ludwigovna':['Ludwigovna'],\n    'Afanasy Ivanovitch Vahrushin':['Vahrushin'],\n    'Sofya Semyonovna Marmeladov':['Sofya Semyonovna','Semyonovna Marmeladov'],\n    'Resslich':['Madame Resslich'],\n    'Alyona Ivanovna': ['pawnbroker'],\n    'Porfiry Petrovitch': ['Porfiry']\n    \n}\n\n# Apply the rules to the index of freq_df\nfor key, values in my_rules.items():\n    for value in values:\n        if value in freq_df.index:\n            freq_df.rename(index={value: key}, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.229032Z","iopub.execute_input":"2023-03-10T11:52:50.229562Z","iopub.status.idle":"2023-03-10T11:52:50.244542Z","shell.execute_reply.started":"2023-03-10T11:52:50.229507Z","shell.execute_reply":"2023-03-10T11:52:50.243361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_df = freq_df.groupby(by=freq_df.index ).sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.245924Z","iopub.execute_input":"2023-03-10T11:52:50.246597Z","iopub.status.idle":"2023-03-10T11:52:50.260447Z","shell.execute_reply.started":"2023-03-10T11:52:50.246559Z","shell.execute_reply":"2023-03-10T11:52:50.259185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.6\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Final Word Cloud</h1>","metadata":{}},{"cell_type":"code","source":"# select the 15 most important characters\nfreq_df = freq_df.sort_values(by = 'freq', ascending= True).iloc[-15:]\nfreq_df = freq_df.sort_values(by = 'freq', ascending= False)\nfreq_df","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.262307Z","iopub.execute_input":"2023-03-10T11:52:50.263289Z","iopub.status.idle":"2023-03-10T11:52:50.288398Z","shell.execute_reply.started":"2023-03-10T11:52:50.263193Z","shell.execute_reply":"2023-03-10T11:52:50.286964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8)) ###\nfreq_df = freq_df.apply(lambda x: x*2)\nwordcloud(freq_df['freq'], title='Characters',max_words=20)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.290664Z","iopub.execute_input":"2023-03-10T11:52:50.291176Z","iopub.status.idle":"2023-03-10T11:52:50.824621Z","shell.execute_reply.started":"2023-03-10T11:52:50.291134Z","shell.execute_reply":"2023-03-10T11:52:50.823301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_custom(freq_df, 'Character Name','freq')","metadata":{"execution":{"iopub.status.busy":"2023-03-10T11:52:50.827035Z","iopub.execute_input":"2023-03-10T11:52:50.827407Z","iopub.status.idle":"2023-03-10T11:52:51.194908Z","shell.execute_reply.started":"2023-03-10T11:52:50.827375Z","shell.execute_reply":"2023-03-10T11:52:51.193536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.7\"></a>\n<h1 style=\"background-color:#33001a;font-family:cursive;font-size:150%;color:#fff;text-align:center;border-radius:40px;height:40px;line-height:40px;\">Reference</h1>","metadata":{}},{"cell_type":"markdown","source":"\n[Spacy Library]('https://spacy.io/') \n\n[TextAcy]('https://pypi.org/project/textacy/')\n\n[Wordcloud]('https://pypi.org/project/wordcloud/')\n\n[Hands On Natural Language Processing (NLP) using Python]('https://www.udemy.com/course/hands-on-natural-language-processing-using-python/') \n\n[Blueprints  for Text Analytics Using Python]('https://www.oreilly.com/library/view/blueprints-for-text/9781492074076/') \n\n[Regex101]('https://regex101.com/r/rX1tE6/7') \n\n[Beautiful bar plots matplotlib]('https://scentellegher.github.io/visualization/2018/10/10/beautiful-bar-plots-matplotlib.html') ","metadata":{}}]}